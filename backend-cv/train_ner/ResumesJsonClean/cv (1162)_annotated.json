{
  "text": "Cheat Sheet www.mapdb.org\nMaven\n<dependency>\n    <groupId>org.mapdb</groupId>\n    <artifactId>mapdb</artifactId>\n    <version>[version]</version>\n</dependency>\nMap stored in file\nimport org.mapdb.*;        \n//open (or create) database \nFile file = new File(“dbFileName”);\nDB db = DBMaker\n.newFileDB(file)\n.make(); \n//use map\nMap map = db.getHashMap(“mapName”);\nmap.put(“aa”,”bb”);  \n//commit and close database\ndb.commit();\ndb.close();\nIn-memory off-heap Map\n// same as above except different method\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \nIn-memory off-heap Queue\n// same as above except different method\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \nQueue q = db.getQueue(“q”);Options to make it faster\nDB db = DBMaker\n// all options works with files as well\n  .newMemoryDB();          \n// disable transactions make writes\n// but you may lose data if store crashes\n  .transactionsDisable()    \n// memory mapped files are faster\n// but does not work well on 32bit\n  .mmapFileEnable()        \n// writes done by background thread\n// it has some overhead, so could be slower\n  .asyncWriteEnable()       \n// increase cache size if you have free heap\n// default value is 32K\n  .cacheSize(1000000)     \n  .make(); \nOther DBMaker options\n// encrypt data with password  \n  .encryptionEnable(“password”) \n// use fast compression \n  .compressionEnable()   \n// enables CRC32 checksum \n// to protect from data corruption\n  .checksumEnable()       \nCache options\n// It caches deserialized objects on heap.\n// Default cache size is 32,000, increase it\n  .cacheSize(1000000)     \n// enable least-recently-used cache \n  .cacheLRUEnable()\n// Unbounded soft-reference cache\n// use with plenty of free heap\n  .cacheSoftRefEnable() // Hard ref, use if \nheap is larger then store\n  .cacheHardRefEnable()Concurrent transactions\n// By default there is single-global \n// transaction per store. \n// This enables proper transactions \n// with full serializable isolation\nTxMaker txMaker = DBMaker\n.newFileDB(file)\n          .makeTxMaker();\n// open two transactions, with single map\n// both can only see their own changes\nDB tx1 = txMaker.makeTx();\nMap map1 = tx1.getTreeMap(\"map\");\nDB tx2 = txMaker.makeTx();\nMap map2 = tx2.getTreeMap(\"map\");\n//commit and close\ntx1.commit()\ntx2.commit()\ntxMaker.close()\nSnapshots\n// lighter way to get consistent data view\nDB  db = DBMaker\n.newFileDB(file)\n.snapshotEnable()\n.make()\nMap map = db.getHashMap(“map”);\nmap.put(1,2);\nDB snap = db.snapshot();\nMap mapOld = snap.getHashMap(“map”);\nmap.put(3,4);   //mapOld still has only 1,2\nsnap.close();   //release resources\n// Third way to ensure consistency is \n// Compare and Swap operation. MapDB \n// has ConcurrentMap and atomic variables.\nMapDB\nCheat Sheet www.mapdb.org\nMaps and Sets\n// Shows how to get all available collections\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \n// BTreeMap is good for small sorted keys\nConcurrentNavigableMap treeMap =\ndb.getTreeMap(“treeMap”);\n// HashMap (aka HTreeMap) is good for\n// larger keys and as a cache \nConcurrentMap hashMap =\ndb.getHashMap(“hashMap”);\n// there is also TreeSet and HashSet\nSortedSet treeSet = db.getTreeSet(“ts”);\nSet  hashSet = db.getHashSet(“hashSet”);\nQueues\n// first-in-first-out queue\nBlockingQueue fifo = db.getQueue(“fifo”);\n// last-in-first-out queue (stack)\nBlockingQueue lifo = db.getStack(“lifo”);\n// circular queue with limited size\nBlockingQueue c =\n db.getCircularQueue(“circular”);\nAtomic records\n// atomically updated records stored in DB\n// Useful for example for sequential IDs.\n// there is Long,  Integer, String \n// and general atomic variable\nAtomic.Long q =db.getAtomicLong(“long”);\nq.set(1999);\nlong id = q.incremendAndGet();Configuring maps\n// create map optimized for large values\nMap<String,String> m = \n   db.createTreeMap(“treeMap”);\n//serializers are critical for performance\n   .keySerializer(BTreeKeySerializer.STRING) \n// compress large ASCII string values   \n   .valueSerializer(\nnew Serializer.CompressionWrapper(\nSerializer.STRING_ASCII))\n// and store values outside of BTree nodes\n   .valuesOutsideNodesEnable()\n// enable size counter\n   .counterEnable()\n// make BTree nodes larger \n   .nodeSize(120)\n// and finally create map\n   .makeOrGet();\nSecondary indexes\n// create secondary value (1:1 relation)\n// secondary map gets auto updated\nMap<ID, Person> persons \nMap<ID, Branch> branches\nBind.secondaryValue(persons,branches,\n   (person)-> person.getBranch()));\n// create secondary key (index) for age(N:1)\nSortedSet<Fun.Tuple2<Age,ID>> ages\nBind.secondaryKey(persons, ages,\n  (person)-> person.getAge());\n// get all persons of age 32\nfor(ID id: Fun.filter(ages, 32)){\n   Person p = persons.get(id)\n}HTreeMap as a cache\n// Entries are removed if map is too large\n// Off-heap map with max size 16GB\nMap cache = DBMaker\n.newCacheDirect(16)\n// On-disk cache in temp folder \n// with max size 128GB or 1M entries\nDB db = DBMaker\n.newT empFileDB()\n.transactionDisable()\n.closeOnJvmShutdown()\n.deleteFilesAfterClose()\n   .make()\nMap cache = db\n.createHashMap(\"cache\")\n           .expireStoreSize(128)  // GB\n           .expireMaxSize(1000000)\n.make()\nData Pump for faster\nimport\n// Data Pump creates TreeMap and TreeSet \n// in streaming fashion. Import time is linear\n// to number of entries. \nIterator iter = … iterate over keys..\nMap<K,V> m =  db.createTreeMap(\"map\")\n  .pumpSource(iter, (key)-> key.getValue())\n  .pumpIgnoreDuplicates()\n  .pumpPresort(1000000)\n  .make()\nMapDB\n",
  "entities": [
    [
      4467,
      4470,
      "SKILL"
    ],
    [
      3826,
      3837,
      "SKILL"
    ],
    [
      2460,
      2468,
      "SKILL"
    ],
    [
      4486,
      4489,
      "SKILL"
    ],
    [
      5215,
      5219,
      "SKILL"
    ],
    [
      2074,
      2077,
      "SKILL"
    ],
    [
      3392,
      3393,
      "SKILL"
    ],
    [
      2083,
      2086,
      "SKILL"
    ],
    [
      1847,
      1853,
      "SKILL"
    ],
    [
      2612,
      2623,
      "SKILL"
    ],
    [
      3732,
      3738,
      "SKILL"
    ],
    [
      1168,
      1173,
      "SKILL"
    ],
    [
      2691,
      2697,
      "SKILL"
    ],
    [
      3758,
      3759,
      "SKILL"
    ],
    [
      863,
      868,
      "SKILL"
    ],
    [
      1467,
      1473,
      "SKILL"
    ],
    [
      1174,
      1176,
      "SKILL"
    ],
    [
      855,
      859,
      "SKILL"
    ],
    [
      306,
      310,
      "SKILL"
    ],
    [
      370,
      372,
      "SKILL"
    ],
    [
      2579,
      2588,
      "SKILL"
    ],
    [
      1124,
      1129,
      "SKILL"
    ],
    [
      766,
      770,
      "SKILL"
    ],
    [
      1314,
      1318,
      "SKILL"
    ],
    [
      2097,
      2104,
      "SKILL"
    ],
    [
      1319,
      1330,
      "SKILL"
    ],
    [
      3293,
      3298,
      "SKILL"
    ],
    [
      26,
      31,
      "SKILL"
    ],
    [
      5113,
      5122,
      "SKILL"
    ],
    [
      757,
      762,
      "SKILL"
    ],
    [
      2886,
      2890,
      "SKILL"
    ],
    [
      2605,
      2611,
      "SKILL"
    ],
    [
      1870,
      1873,
      "SKILL"
    ],
    [
      226,
      234,
      "SKILL"
    ],
    [
      2741,
      2745,
      "SKILL"
    ],
    [
      317,
      320,
      "SKILL"
    ],
    [
      689,
      696,
      "SKILL"
    ],
    [
      3188,
      3194,
      "SKILL"
    ],
    [
      3437,
      3444,
      "SKILL"
    ],
    [
      1404,
      1419,
      "SKILL"
    ],
    [
      1651,
      1660,
      "SKILL"
    ],
    [
      3681,
      3692,
      "SKILL"
    ],
    [
      705,
      707,
      "SKILL"
    ],
    [
      2789,
      2800,
      "SKILL"
    ],
    [
      439,
      445,
      "SKILL"
    ],
    [
      810,
      822,
      "SKILL"
    ],
    [
      4223,
      4230,
      "SKILL"
    ],
    [
      1130,
      1134,
      "SKILL"
    ],
    [
      956,
      960,
      "SKILL"
    ],
    [
      2643,
      2647,
      "SKILL"
    ],
    [
      5189,
      5198,
      "SKILL"
    ],
    [
      668,
      669,
      "SKILL"
    ],
    [
      1409,
      1419,
      "SKILL"
    ],
    [
      2648,
      2657,
      "SKILL"
    ],
    [
      33,
      43,
      "SKILL"
    ],
    [
      4456,
      4461,
      "SKILL"
    ]
  ]
}